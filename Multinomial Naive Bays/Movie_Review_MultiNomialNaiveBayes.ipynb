{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the csv files\n",
    "df=pd.read_csv(\"./datasets/Movies/Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting dataframes-->numpy array\n",
    "data=df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting X and Y data\n",
    "X=data[:,:1]\n",
    "Y=data[:,1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising out TOkenizer,stopwords and Stemmer\n",
    "rg=RegexpTokenizer(r'[a-z@._]+')\n",
    "en_stopwords=set(stopwords.words('english'))\n",
    "ps=PorterStemmer();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting document into bag of words\n",
    "def getstemmedreview(X):\n",
    "    x_clean=[]\n",
    "    for review in X:\n",
    "        review=str(review).lower()\n",
    "        review=review.replace(\"<br /><br />\",\" \")\n",
    "        tokens=rg.tokenize(review)\n",
    "        new_tokens=[token for token in tokens if token not in en_stopwords or token=='not']\n",
    "        stemmed_tokens=[ps.stem(token) for token in new_tokens]\n",
    "        sent=' '.join(stemmed_tokens)\n",
    "        x_clean.append(sent)\n",
    "    print(len(x_clean))\n",
    "    return x_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "x=getstemmedreview(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the values\n",
    "Y=Y.reshape(-1,)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "le.fit(Y)\n",
    "y=le.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating personalised vocab as we have only two labels so I am using vocab_p and vocab_b to save computations\n",
    "def vocab(x):\n",
    "    vocabulary_p={}\n",
    "    vocabulary_n={}\n",
    "    for i in range(len(x)):\n",
    "        reviews=x[i]\n",
    "        review=reviews.split(' ')\n",
    "        if y[i]==1:\n",
    "            for words in review:\n",
    "                if str(words) in vocabulary_p:\n",
    "                    vocabulary_p[words]+=1\n",
    "                else:\n",
    "                    vocabulary_p[words]=1\n",
    "        else:\n",
    "            for words in review:\n",
    "                if words in vocabulary_n:\n",
    "                    vocabulary_n[words]+=1\n",
    "                else:\n",
    "                    vocabulary_n[words]=1\n",
    "    return vocabulary_p,vocabulary_n;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_p,dictionary_n=vocab(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading testing data\n",
    "x_test=pd.read_csv('./datasets/Movies/Test.csv')\n",
    "X_test=x_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Conditional Prob and using laplace smoothing\n",
    "def cond_prob(x_train,feature_value,label):\n",
    "    num=1\n",
    "    denom=len(dictionary_p)\n",
    "    if feature_value in dictionary_p:\n",
    "        denom+=dictionary_p[str(feature_value)]\n",
    "    if feature_value in dictionary_n:   \n",
    "        dictionary_n[str(feature_value)]\n",
    "    if label==1 and feature_value in dictionary_p:\n",
    "        num=dictionary_p[str(feature_value)]\n",
    "    elif feature_value in dictionary_n:\n",
    "        num=dictionary_n[str(feature_value)]\n",
    "    \n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    " x_test=getstemmedreview(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will handle prediction and save all the vlaues in y_pred\n",
    "def predict(x_train,y_train,x_test):\n",
    "    labels=np.unique(y_train)\n",
    "    y_pred=[]\n",
    "    for doc in x_test:\n",
    "        pred=[]\n",
    "        for label in labels:\n",
    "            likelihood=1.0\n",
    "            docx=doc.split(' ')\n",
    "            for feature in docx:\n",
    "                likelihood*=cond_prob(x_train,str(feature),label)\n",
    "            pred.append(likelihood)\n",
    "        y_pred.append(np.argmax(pred))\n",
    "    return y_pred\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=predict(x,y,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to Store output\n",
    "def output(y_pred):\n",
    "    ans='Id,label\\n'\n",
    "    for i in range(len(y_pred)):\n",
    "        ans+=(str(i)+','+ ('pos' if y_pred[i]==1 else 'neg')+'\\n')\n",
    "    with open('movie.csv','w') as f:\n",
    "        f.write(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying Using Sklearn and TfIDf normalisation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating objects for the classes\n",
    "mnb=MultinomialNB()\n",
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fir_transform the training data and transform the testing data as it will keep the shape intact\n",
    "x_vec=tfidf.fit_transform(x)\n",
    "x_vec_test=tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(x_vec,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(x_vec,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=mnb.predict(x_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "output(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
